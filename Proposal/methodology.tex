\chapter{Proposed Methodology}
\label{cha:methodology}

The methods for this project are largely expected to be a back-and-forth process between creating/modifying spectral libraries as training data, inputting these into \tc to train the model, and testing model output using various statistical diagnostics. This chapter will individually discuss these three steps, but it is important to note that this is not a strictly linear methodology, and I am expecting to undertake each step multiple times throughout this project.

Section \ref{sec:libraries} will cover the process of developing a training set based on a chosen galaxy parametrisation label. As I have already started compiling a spectral library with varying \gls{sfh}, direct examples will be included. Next, I will outline the process of training \tc using these training sets in Section \ref{sec:training}. This section will also explain the process of assessing the performance of \tc in retrieving labels from input spectra. I will finally briefly discuss the process of adding parametrisation labels to the model, and potential issues and contingencies associated with increasing complexity, in Section \ref{sec:addlabels}.

\section{Developing a Training Set}
\label{sec:libraries}

The creation of a training set of galaxy spectra for \tc will largely be done using the Python library \gls{fsps} \citep{conroy_propagation_2009, conroy_propagation_2010}. The command \texttt{fsps.StellarPopulation()} initialises an \gls{ssp} based on input parameters. In the most basic case with maximum stochasticity, this is simply called as follows:
\begin{minted}[frame=single, framesep=2mm]{python}
    sp = fsps.StellarPopulation(
            sfh = 0, # single stellar population
            imf_type = 1, # Chabrier IMF
            nebemlineinspec = False # turn off nebular emission
        )
\end{minted}
By default, \gls{fsps} will initialise this stellar population for an array of 94 values of elapsed time since the start of the universe $(t_\mathrm{age})$, spaced uniformly on a logarithmic scale from $10^{5.5}$ to $10^{10.15}$ years. Then, running the command \texttt{sp.get\_spectrum()} will return wavelength (in angstroms) and flux (in $L_\odot/\mathrm{Hz}$) of \gls{ssp} spectra for each of these 94 epochs, normalised for stellar formation with a total mass of $1\mathrm{M}_\odot$. These spectra are then trimmed to a wavelength range of around 3000-9000\ang to focus on the visible portion of galaxy \glspl{sed}.

When considering each \gls{ssp} as a component of a galaxy's \gls{sfh}, the output spectra from \gls{fsps} then need to be binned to a more realistic resolution. That is, somewhere between five and ten bursts of star formation would be far more computationally viable to model than the 94 given by \gls{fsps}. As such, I define a new set of bins, also uniformly spaced in $t_\mathrm{age}$ on a logarithmic grid, for which the average galaxy spectrum is found.

Finally, these \glspl{ssp} are transformed into a galaxy spectrum with a by assigning a dividing each spectrum by the number of \glspl{ssp}, and then summing them all. Of course, in this basic scenario, each \gls{ssp} is weighted the same so the resultant galaxy spectrum is just a straightforward sum of \glspl{ssp} generated from the same parameters. To actually test the effect of varying \gls{ssp} parameters on the final galaxy spectrum an extra step will need to be added before summing all the \gls{ssp} spectra. The simplest way to do this is to make \gls{sfh} the first label for \tc's training set.

\subsection{Varying the SFH Parameter}

A galaxy's \gls{sfh} can be taken to be a linear combination of \glspl{ssp}. As such, a \gls{sfh} can be generated by simply assigning fractional ``weights'' to each \gls{ssp}, such that the weight determines the relative impact of a particular star formation event on the galaxy's present-day spectrum. As discussed in Chapter \ref{cha:lit-review}, \glspl{sfh} have been characterised by many functional forms, but in the interest of minimising bias in the training set and maximising stochasticity, I opted for weights to be generated randomly on a Dirichlet distribution, using a uniformly distributed alpha variable.

Thousands of synthetic galaxy spectra can thus be generated, building up a training library for \tc to learn relationships between \gls{sfh} and galaxy \gls{sed}\footnote{My program for generating these spectra can be found on \href{https://gist.github.com/vernica-mehta/8eadb727fdd0601a868df7271c0a5e3a}{GitHub}.}. The final step in preparing the training library for \tc is continuum-normalisation, which can be done by dividing the spectra by a moving average, determined from wavelength bins of the spectrum. Fig. \ref{fig:sample-spec} provides an example of one such spectrum generated using the procedure described above.

\begin{figure}[!h]
    \centering
    \includegraphics[width=\textwidth]{Proposal/figs/sample-spec.pdf}
    \caption{Example galaxy spectrum generated by synthesising ten randomly-weighted \glspl{ssp}, using procedures described in Section \ref{sec:libraries}. The top panel gives the original spectrum and the bottom panel is the flattened continuum-normalised version.}
    \label{fig:sample-spec}
\end{figure}

\section{Training and Testing}
\label{sec:training}

With a training library of spectra generated, the next step will be to train \tc to learn the relationships between \gls{sfh} and the galaxy spectrum. For this to be as effective as possible, however, I would first need to optimise for the number of \glspl{ssp} forming the galaxy \gls{sfh}. This means compiling training libraries of spectra generated using different numbers of weighted bins, to determine which version results in the best performance of the model. An additional measure would be to add synthetically-generated noise to training data, in an effort to mimic real galaxy spectra. 

\subsection{Assessing Model Performance}

$k$-fold cross-validation techniques will be used to assess the performance of the model in retrieving \gls{sfh} from galaxy spectra generated using different numbers of \glspl{ssp}. This method, also known as the Predictive Sample Reuse Method \citep{geisser_predictive_1975}, involves the partitioning of data into $k$ disjoint subsets (folds). Each fold iteratively serves as a test set, while the remaining $k-1$ folds comprise the training set, where final performance is estimated by averaging across all $k$ iterations. Variations of this method, as described by \citet{aguilar-ruiz_irredundant_2025}, include nested cross-validation and leave-one-out cross-validation which demonstrate greater reliability at greater computational expense.

Implementing $k$-fold cross-validation in Python can be done using tools from the \texttt{sklearn.model\_selection} library, such as \texttt{KFold} and \texttt{cross\_val\_score}. These tools can be easily combined with other machine-learning algorithms to determine their performance.

\subsection{Determining Goodness of Match}

In addition to $k$-fold cross-validation techniques which assess the performance of a model as a whole, an extra step can be added to compare the goodness of match between an input \gls{sfh} and output \gls{sfh} resolved from the spectrum generated by the input \gls{sfh}. To test the quality of a modelled \gls{sfh} against an original \gls{sfh}, a least-squares fitting approach can be taken \citep{zhang_testing_2025}:
\begin{equation}
    \Delta_\text{SFH}=\frac{\sigma}{\mu}
\end{equation}
Where $\mu$ is the average \gls{sfr} over the \gls{sfh}, given by
\begin{equation}
    \mu\equiv\frac{1}{N}\sum_{i=1}^N\text{SFR}_\text{input,i}
\end{equation}
And $\sigma$ is the variance between the input \gls{sfh} and best-fit model \gls{sfh}, given by
\begin{equation}
    \sigma\equiv\frac{1}{N}\sum_{i=1}^N\left(\text{SFR}_\text{input,i}-\text{SFR}_\text{model,i}\right)^2
\end{equation}
Where $N$ is the total number of time bins used to represent the \gls{sfh}.

\section{Adding Labels, Increasing Complexity}
\label{sec:addlabels}

For parameters that affect the inherent composition of a \gls{ssp}, training sets will need to be constructed different. One option would be to vary \gls{fsps} inputs, allowing separate initialisation for each \gls{ssp}. However, this will significantly increase computational time required to generate training libraries. The other option is to initialise the most basic stellar population in \gls{fsps} as before, and introduce functional variations to \glspl{ssp} afterwards, analogous to ``weights'' used to characterise \gls{sfh}. This would require a high level of consideration to weigh up all the dependencies between parameters, but could be less computationally expensive.

A potential obstacle in this methodology would be the continuum-normalisation of all spectra for \tc. Aspects of galaxy parametrisation, such as the effects of dust, affect the spectral continuum rather than emission and absorption features \citep{calzetti_effects_2001, kriek_dust_2013}. Even if a training set of spectra was constructed for varying dust parameters, the process of flattening the \glspl{sed} would negate any distinguishing features. In this case, decisions would have to be made about whether the dust parameter should be included, and if so, how forgoing continuum-normalisation might distort \tc's machine-learning process.

A further challenge would be considering the intersecting effects of simultaneously varying parameters, as is the case in actual galaxy \glspl{sed}, rather than the simple case of training \tc with datasets that change one variable at a time. Of course, training sets with multiple free parameters can be generated using a combination of \gls{fsps} inputs and external functions, but it will remain a matter for further investigation whether the size of the training library would need to be much larger to account for greater variations.