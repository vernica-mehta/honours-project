\chapter{Proposed Methodology}
\label{cha:methodology}

The methods for this project are largely expected to be a back-and-forth process between creating/modifying spectral libraries as training data, inputting these into \tc to train the model, and testing model output using various statistical diagnostics. This chapter will individually discuss these three steps, but it is important to note that this is not a strictly linear methodology, and I am expecting to undertake each step multiple times throughout this project.

Section \ref{sec:libraries} will cover the process of developing a training set based on a chosen galaxy parametrisation label. As I have already started compiling a spectral library with varying \gls{sfh}, direct examples will be included. Next, I will outline the process of training \tc using these training sets in Section \ref{sec:training}. Section \ref{sec:testing} will then outline the process of assessing the performance of \tc in retrieving labels from input spectra. I will finally briefly discuss the process of adding parametrisation labels to the model, and potential issues and contingencies associated with increasing complexity, in Section \ref{sec:addlabels}.

\section{Developing a Training Set}
\label{sec:libraries}

The creation of a training set of galaxy spectra for \tc will largely be done using the Python library \gls{fsps} \citep{conroy_propagation_2009, conroy_propagation_2010}. The command \texttt{fsps.StellarPopulation()} initialises an \gls{ssp} based on input parameters. In the most basic case with maximum stochasticity, this is simply called as follows:
\begin{minted}[frame=single, framesep=2mm]{python}
    sp = fsps.StellarPopulation(
            sfh = 0, # single stellar population
            imf_type = 1, # Chabrier IMF
            nebemlineinspec = False # turn off nebular emission
        )
\end{minted}
By default, \gls{fsps} will initialise this stellar population for an array of 94 values of elapsed time since the start of the universe $(t_\mathrm{age})$, spaced uniformly on a logarithmic scale from $10^{5.5}$ to $10^{10.15}$ years. Then, running the command \texttt{sp.get\_spectrum()} will return wavelength (in angstroms) and flux (in $L_\odot/\mathrm{Hz}$) of \gls{ssp} spectra for each of these 94 epochs, normalised for stellar formation with a total mass of $1\mathrm{M}_\odot$. These spectra are then trimmed to a wavelength range of around 3000-9000\ang to focus on the visible portion of galaxy \glspl{sed}.

When considering each \gls{ssp} as a component of a galaxy's \gls{sfh}, the output spectra from \gls{fsps} then need to be binned to a more realistic resolution. That is, somewhere between five and ten bursts of star formation would be far more computationally viable to model than the 94 given by \gls{fsps}. As such, I define a new set of bins, also uniformly spaced in $t_\mathrm{age}$ on a logarithmic grid, for which the average galaxy spectrum is found.

Finally, these \glspl{ssp} are transformed into a galaxy spectrum with a by assigning a dividing each spectrum by the number of \glspl{ssp}, and then summing them all. Of course, in this basic scenario, each \gls{ssp} is weighted the same so the resultant galaxy spectrum is just a straightforward sum of \glspl{ssp} generated from the same parameters. To actually test the effect of varying \gls{ssp} parameters on the final galaxy spectrum an extra step will need to be added before summing all the \gls{ssp} spectra. The simplest way to do this is to make \gls{sfh} the first label for \tc's training set.

\subsection{Varying the SFH Parameter}

A galaxy's \gls{sfh} can be taken to be a linear combination of \glspl{ssp}. As such, a \gls{sfh} can be generated by simply assigning fractional ``weights'' to each \gls{ssp}, such that the weight determines the relative impact of a particular star formation event on the galaxy's present-day spectrum. As discussed in Chapter \ref{cha:lit-review}, \glspl{sfh} have been characterised by many functional forms, but in the interest of minimising bias in the training set and maximising stochasticity, I opted for weights to be generated randomly on a Dirichlet distribution, using a uniformly distributed alpha variable.

Thousands of synthetic galaxy spectra can thus be generated, building up a training library for \tc to learn relationships between \gls{sfh} and galaxy \gls{sed}\footnote{My program for generating these spectra can be found on \href{https://gist.github.com/vernica-mehta/8eadb727fdd0601a868df7271c0a5e3a}{GitHub}.}.

\section{Training Up \textit{The Cannon}}
\label{sec:training}

\section{Statistical Testing}
\label{sec:testing}

\subsection{Determining Goodness of Match}

To test the quality of a modelled \gls{sfh} against an original \gls{sfh}, a least-squares fitting approach can be taken \citep{zhang_testing_2025}:
\begin{equation}
    \Delta_\text{SFH}=\frac{\sigma}{\mu}
\end{equation}
Where $\mu$ is the average \gls{sfr} over the \gls{sfh}, given by
\begin{equation}
    \mu\equiv\frac{1}{N}\sum_{i=1}^N\text{SFR}_\text{input,i}
\end{equation}
And $\sigma$ is the variance between the input \gls{sfh} and best-fit model \gls{sfh}, given by
\begin{equation}
    \sigma\equiv\frac{1}{N}\sum_{i=1}^N\left(\text{SFR}_\text{input,i}-\text{SFR}_\text{model,i}\right)^2
\end{equation}
Where $N$ is the total number of time bins used to represent the \gls{sfh}.

\section{Adding Labels, Increasing Complexity}
\label{sec:addlabels}